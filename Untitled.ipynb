{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install GetOldTweets3\n",
    "!{sys.executable} -m pip install tweet-preprocessor\n",
    "!{sys.executable} -m pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import GetOldTweets3 as got\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import preprocessor as p\n",
    "import string\n",
    "import emoji\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove urls from tweets\n",
    "\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "\n",
    "# Number of tweets to retrieve\n",
    "count = 500\n",
    "\n",
    "# List of tweets retrieved\n",
    "tweet_list = []\n",
    "\n",
    "# Dictionary to which the tweets are added\n",
    "def tweet_to_dict(tweet):\n",
    "    tweet_dict = {\n",
    "        'user': tweet.username,\n",
    "        'id': tweet.id,\n",
    "        ## Fixed time from UTC to UTC+3\n",
    "        'date': str(tweet.date+timedelta(hours=3)),\n",
    "        'hashtags': tweet.hashtags,\n",
    "        'text': remove_url(tweet.text),\n",
    "    }\n",
    "    return tweet_dict\n",
    "\n",
    "# Adding tweets to dictionary and then to list\n",
    "def add_to_list():\n",
    "    for tweet in tweets:\n",
    "        tweet_dict_temp = tweet_to_dict(tweet)\n",
    "        tweet_list.append(tweet_dict_temp)\n",
    "    print('--- Added to list!--- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./data/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in files:\n",
    "    cdf = pd.read_csv(f)\n",
    "    df = df.append(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20610948, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>loc</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-27 00:00:00+00:00</td>\n",
       "      <td>NE</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>omaha metropolitan healthcare coalition report...</td>\n",
       "      <td>u1382953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-27 00:00:00+00:00</td>\n",
       "      <td>WI</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>zamundatwice that corona even breathed were de...</td>\n",
       "      <td>u146449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-27 00:00:00+00:00</td>\n",
       "      <td>AR</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>holiday weekend crowds trigger covid warnings ...</td>\n",
       "      <td>u1261797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-27 00:00:00+00:00</td>\n",
       "      <td>MD</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>pennsylvania world veteran been walking laps r...</td>\n",
       "      <td>u1336641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-27 00:00:00+00:00</td>\n",
       "      <td>ID</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>icymi once campus boise state athletes will un...</td>\n",
       "      <td>u1416930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at loc  sentiment  subjectivity  \\\n",
       "0  2020-05-27 00:00:00+00:00  NE       0.00      0.750000   \n",
       "1  2020-05-27 00:00:00+00:00  WI      -0.25      0.400000   \n",
       "2  2020-05-27 00:00:00+00:00  AR       0.00      0.000000   \n",
       "3  2020-05-27 00:00:00+00:00  MD       0.00      0.000000   \n",
       "4  2020-05-27 00:00:00+00:00  ID       0.00      0.076923   \n",
       "\n",
       "                                                text   user_id  verified  \n",
       "0  omaha metropolitan healthcare coalition report...  u1382953         0  \n",
       "1  zamundatwice that corona even breathed were de...   u146449         0  \n",
       "2  holiday weekend crowds trigger covid warnings ...  u1261797         0  \n",
       "3  pennsylvania world veteran been walking laps r...  u1336641         1  \n",
       "4  icymi once campus boise state athletes will un...  u1416930         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting preprocesor options\n",
    "p.set_options(p.OPT.URL,p.OPT.MENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It removes URLs and mentions\n",
    "def preprocess_tweet(row):\n",
    "    text = row['text']\n",
    "    text = p.clean(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demojize\n",
    "# Replaces emojis with text\n",
    "def demojize(row):\n",
    "    text = row['text']\n",
    "    text = emoji.demojize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying previously defined functions\n",
    "df['text'] = df.apply(preprocess_tweet, axis=1)\n",
    "df['text'] = df.apply(demojize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicate tweets\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df['data'] = df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining stopwords and adding a space before and after to exclude the case\n",
    "# in which the stopword is cointained in a word\n",
    "words = set(stopwords.words('english'))\n",
    "stopwords = [' ' + x + ' ' for x in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the tags\n",
    "tweets.text = tweets.text.replace(\n",
    "    \"@[\\w]*[_-]*[\\w]*\", \" \", regex=True)\n",
    "# Removing the spaces in excess\n",
    "tweets.text = tweets.text.replace('\\s+', ' ', regex=True)\n",
    "# Removing the space at the beginning\n",
    "tweets.text = tweets.text.replace('^ ', '', regex=True)\n",
    "# Removing the space at the end\n",
    "tweets.text = tweets.text.replace(' $', '', regex=True)\n",
    "# To lowercase\n",
    "tweets.text = tweets.text.apply(\n",
    "    lambda x: x.lower())\n",
    "tweets.text = tweets.text.replace('^', ' ', regex=True)\n",
    "tweets.text = tweets.text.replace('$', ' ', regex=True)\n",
    "\n",
    "# Removing stopwords\n",
    "for word in stopwords:\n",
    "    tweets.text = tweets.text.replace(word, ' ', regex=True)\n",
    "\n",
    "# Removing the space at the beginning and at the end\n",
    "tweets.text = tweets.text.apply(lambda x: x.strip())\n",
    "# Removing empty tweets\n",
    "tweets = tweets[tweets.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grebprog</td>\n",
       "      <td>1302219720767213568</td>\n",
       "      <td>2020-09-05 15:19:10+00:00</td>\n",
       "      <td></td>\n",
       "      <td>key coronavirus forecast predicts 410000 total...</td>\n",
       "      <td>Key coronavirus forecast predicts over 410000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wwolfhunt</td>\n",
       "      <td>1302219716086435840</td>\n",
       "      <td>2020-09-05 15:19:09+00:00</td>\n",
       "      <td></td>\n",
       "      <td>sickness worrying coronavirus incompetence van...</td>\n",
       "      <td>The sickness you should be worrying about is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TriviaAi</td>\n",
       "      <td>1302219710419722240</td>\n",
       "      <td>2020-09-05 15:19:07+00:00</td>\n",
       "      <td></td>\n",
       "      <td>countries ban americans visiting since shithol...</td>\n",
       "      <td>Most countries ban Americans from visiting sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sharonbabes1</td>\n",
       "      <td>1302219708972896256</td>\n",
       "      <td>2020-09-05 15:19:07+00:00</td>\n",
       "      <td>#coronavirus</td>\n",
       "      <td>well done clap spreading coronavirus arses</td>\n",
       "      <td>Well done a clap for spreading coronavirus arses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worldbankchina</td>\n",
       "      <td>1302219707488129024</td>\n",
       "      <td>2020-09-05 15:19:06+00:00</td>\n",
       "      <td>#COVID19 #coronavirus #WorldBankPodcast</td>\n",
       "      <td>covid19 coronavirus potential strain healthcar...</td>\n",
       "      <td>COVID19 coronavirus has the potential to strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stevebeasant</td>\n",
       "      <td>1302219683119140864</td>\n",
       "      <td>2020-09-05 15:19:01+00:00</td>\n",
       "      <td>#Coronavirus</td>\n",
       "      <td>coronavirus new local lockdown rules</td>\n",
       "      <td>Coronavirus What are the new local lockdown rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LindaGMan1</td>\n",
       "      <td>1302219679080099840</td>\n",
       "      <td>2020-09-05 15:19:00+00:00</td>\n",
       "      <td>#Coronavirus #Covid19 #Health #HealthNews #Sci...</td>\n",
       "      <td>vitamin deficiency could linked increased risk...</td>\n",
       "      <td>Vitamin D deficiency could be linked to increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MrASTM</td>\n",
       "      <td>1302219671769427968</td>\n",
       "      <td>2020-09-05 15:18:58+00:00</td>\n",
       "      <td>#FoxNews</td>\n",
       "      <td>left us entirely moved places like italy switz...</td>\n",
       "      <td>there have been some who have left the US enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tribunephl</td>\n",
       "      <td>1302219662235557888</td>\n",
       "      <td>2020-09-05 15:18:56+00:00</td>\n",
       "      <td></td>\n",
       "      <td>1pacman partylist representative eric pineda c...</td>\n",
       "      <td>JUST IN 1PACMAN partylist Representative Eric ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bluewave8yrs</td>\n",
       "      <td>1302219662084763648</td>\n",
       "      <td>2020-09-05 15:18:56+00:00</td>\n",
       "      <td></td>\n",
       "      <td>rochester police cant keep stories straight fi...</td>\n",
       "      <td>Rochester police cant keep their stories strai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                   id                      date  \\\n",
       "0        grebprog  1302219720767213568 2020-09-05 15:19:10+00:00   \n",
       "1       Wwolfhunt  1302219716086435840 2020-09-05 15:19:09+00:00   \n",
       "2        TriviaAi  1302219710419722240 2020-09-05 15:19:07+00:00   \n",
       "3    sharonbabes1  1302219708972896256 2020-09-05 15:19:07+00:00   \n",
       "4  worldbankchina  1302219707488129024 2020-09-05 15:19:06+00:00   \n",
       "5    stevebeasant  1302219683119140864 2020-09-05 15:19:01+00:00   \n",
       "6      LindaGMan1  1302219679080099840 2020-09-05 15:19:00+00:00   \n",
       "7          MrASTM  1302219671769427968 2020-09-05 15:18:58+00:00   \n",
       "8      tribunephl  1302219662235557888 2020-09-05 15:18:56+00:00   \n",
       "9    bluewave8yrs  1302219662084763648 2020-09-05 15:18:56+00:00   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                       #coronavirus   \n",
       "4            #COVID19 #coronavirus #WorldBankPodcast   \n",
       "5                                       #Coronavirus   \n",
       "6  #Coronavirus #Covid19 #Health #HealthNews #Sci...   \n",
       "7                                           #FoxNews   \n",
       "8                                                      \n",
       "9                                                      \n",
       "\n",
       "                                                text  \\\n",
       "0  key coronavirus forecast predicts 410000 total...   \n",
       "1  sickness worrying coronavirus incompetence van...   \n",
       "2  countries ban americans visiting since shithol...   \n",
       "3         well done clap spreading coronavirus arses   \n",
       "4  covid19 coronavirus potential strain healthcar...   \n",
       "5               coronavirus new local lockdown rules   \n",
       "6  vitamin deficiency could linked increased risk...   \n",
       "7  left us entirely moved places like italy switz...   \n",
       "8  1pacman partylist representative eric pineda c...   \n",
       "9  rochester police cant keep stories straight fi...   \n",
       "\n",
       "                                                data  \n",
       "0  Key coronavirus forecast predicts over 410000 ...  \n",
       "1  The sickness you should be worrying about is c...  \n",
       "2  Most countries ban Americans from visiting sin...  \n",
       "3   Well done a clap for spreading coronavirus arses  \n",
       "4  COVID19 coronavirus has the potential to strai...  \n",
       "5  Coronavirus What are the new local lockdown rules  \n",
       "6  Vitamin D deficiency could be linked to increa...  \n",
       "7  there have been some who have left the US enti...  \n",
       "8  JUST IN 1PACMAN partylist Representative Eric ...  \n",
       "9  Rochester police cant keep their stories strai...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing usernames in case preprocessor didn't work\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "# Remove punctuation in case Re missed something\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing mentions\n",
    "tweets['text'] = tweets['text'].apply(remove_mentions)\n",
    "\n",
    "# Removing punctuation\n",
    "tweets['text'] = tweets['text'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accents in order to mitigate weird charachters in the final result\n",
    "def strip_accents(row):\n",
    "    text = row['text']\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing accent\n",
    "tweets['text'] = tweets.apply(strip_accents, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to CSV\n",
    "tweets.to_csv('dataCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /Users/shternev/opt/anaconda3/lib/python3.7/site-packages (0.23.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/shternev/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /Users/shternev/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/shternev/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/shternev/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shternev/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/shternev/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/shternev/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator DictVectorizer from version 0.22.2.post1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install -U scikit-learn\n",
    "# Loading model with pickle\n",
    "with open('Models/LRClassifier.pkl', 'rb') as f:\n",
    "    LRClassifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tweets from April\n",
    "tweets = pd.read_csv('dataCleaned.csv', usecols=[\n",
    "    'data', 'text', 'date'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# English stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Italian Stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "# # Additional stopwords found online\n",
    "# def additional_stop_words():\n",
    "#     with open('Training\\\\stopwords.txt', 'r') as f:\n",
    "#         additional_stopwords = f.readlines()\n",
    "#     additional_stopwords = [x.strip() for x in additional_stopwords]\n",
    "#     return additional_stopwords\n",
    "\n",
    "\n",
    "# Function to remove noise from tokens, removing also stopwords\n",
    "def remove_noise(tweet_tokens, stop_words=(), additional_stop_words=()):\n",
    "    cleaned_tokens = []\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) > 1 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding tweets and tokenized\n",
    "april = tweets.data.values.tolist()\n",
    "tokenized = tweets.text.values.tolist()\n",
    "date = tweets.date.values.tolist()\n",
    "# List of classified tweets from april\n",
    "classified = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each tweet, remove noise and tokenize, calculate the accuracy of a given prediction\n",
    "# and add to classified list\n",
    "for tweet in april:\n",
    "    custom_tokens = remove_noise(word_tokenize(tweet))\n",
    "    classified.append(tuple((tweet, LRClassifier.prob_classify(\n",
    "        dict([token, True] for token in custom_tokens)).prob('Positive'), LRClassifier.prob_classify(\n",
    "        dict([token, True] for token in custom_tokens)).prob('Negative'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating dataframe from classified tweets\n",
    "df = pd.DataFrame(classified, columns=['tweet', 'positive', 'negative'])\n",
    "\n",
    "# Obtain polarity by subtracting positives with negatives values\n",
    "df['polarity'] = df['positive'] - df['negative']\n",
    "\n",
    "# Adding tokenized column\n",
    "df['tokenized'] = tokenized\n",
    "\n",
    "# Adding date column\n",
    "df['date'] = date\n",
    "\n",
    "# Reordering columns\n",
    "df = df[['date', 'tweet', 'tokenized', 'positive', 'negative', 'polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-05 15:19:10+00:00</td>\n",
       "      <td>Key coronavirus forecast predicts over 410000 ...</td>\n",
       "      <td>key coronavirus forecast predicts 410000 total...</td>\n",
       "      <td>0.524976</td>\n",
       "      <td>0.475024</td>\n",
       "      <td>0.049953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-05 15:19:09+00:00</td>\n",
       "      <td>The sickness you should be worrying about is c...</td>\n",
       "      <td>sickness worrying coronavirus incompetence van...</td>\n",
       "      <td>0.600066</td>\n",
       "      <td>0.399934</td>\n",
       "      <td>0.200132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-05 15:19:07+00:00</td>\n",
       "      <td>Most countries ban Americans from visiting sin...</td>\n",
       "      <td>countries ban americans visiting since shithol...</td>\n",
       "      <td>0.347580</td>\n",
       "      <td>0.652420</td>\n",
       "      <td>-0.304840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-05 15:19:07+00:00</td>\n",
       "      <td>Well done a clap for spreading coronavirus arses</td>\n",
       "      <td>well done clap spreading coronavirus arses</td>\n",
       "      <td>0.649369</td>\n",
       "      <td>0.350631</td>\n",
       "      <td>0.298738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-05 15:19:06+00:00</td>\n",
       "      <td>COVID19 coronavirus has the potential to strai...</td>\n",
       "      <td>covid19 coronavirus potential strain healthcar...</td>\n",
       "      <td>0.528693</td>\n",
       "      <td>0.471307</td>\n",
       "      <td>0.057385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2020-09-05 15:19:10+00:00   \n",
       "1  2020-09-05 15:19:09+00:00   \n",
       "2  2020-09-05 15:19:07+00:00   \n",
       "3  2020-09-05 15:19:07+00:00   \n",
       "4  2020-09-05 15:19:06+00:00   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  Key coronavirus forecast predicts over 410000 ...   \n",
       "1  The sickness you should be worrying about is c...   \n",
       "2  Most countries ban Americans from visiting sin...   \n",
       "3   Well done a clap for spreading coronavirus arses   \n",
       "4  COVID19 coronavirus has the potential to strai...   \n",
       "\n",
       "                                           tokenized  positive  negative  \\\n",
       "0  key coronavirus forecast predicts 410000 total...  0.524976  0.475024   \n",
       "1  sickness worrying coronavirus incompetence van...  0.600066  0.399934   \n",
       "2  countries ban americans visiting since shithol...  0.347580  0.652420   \n",
       "3         well done clap spreading coronavirus arses  0.649369  0.350631   \n",
       "4  covid19 coronavirus potential strain healthcar...  0.528693  0.471307   \n",
       "\n",
       "   polarity  \n",
       "0  0.049953  \n",
       "1  0.200132  \n",
       "2 -0.304840  \n",
       "3  0.298738  \n",
       "4  0.057385  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe to csv\n",
    "\n",
    "df.to_csv('april_analyzed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tweets from April\n",
    "tweets = pd.read_csv('betsentiment-EN-tweets-sentiment-worldcup.csv', usecols=[\n",
    "    'tweet_text', 'sentiment'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fifaworldcup_fr @MBaye9Niang @FIFAWorldCupSEN...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just bought a new Socceroos jersey. At �60 i...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@youngy18 @England You look good man I'd appre...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SkySportsNews @England Henderson should be le...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Maguire absolutely towering in second ha...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text sentiment\n",
       "0  @fifaworldcup_fr @MBaye9Niang @FIFAWorldCupSEN...   NEUTRAL\n",
       "1  I just bought a new Socceroos jersey. At �60 i...  POSITIVE\n",
       "2  @youngy18 @England You look good man I'd appre...  POSITIVE\n",
       "3  @SkySportsNews @England Henderson should be le...   NEUTRAL\n",
       "4  Harry Maguire absolutely towering in second ha...  POSITIVE"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
